{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilearn import image, input_data, plotting\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.plotting import plot_stat_map\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from nilearn.datasets import fetch_atlas_harvard_oxford\n"
   ],
   "id": "56f273aa1ece8a4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ## Setup Project Directory Structure\n",
   "id": "22a9164f87def5f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create directory structure\n",
    "project_dir = Path('fmri_project')\n",
    "checkpoints_dir = project_dir / 'checkpoints'\n",
    "results_dir = project_dir / 'results'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [checkpoints_dir, results_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n"
   ],
   "id": "91a3756b6f719cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## Create Events DataFrame\n",
    "# Create events DataFrame from the timing information\n"
   ],
   "id": "d84c28ec2ad1f24f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_checkpoint(data, filename):\n",
    "    \"\"\"Save data to checkpoint file\"\"\"\n",
    "    filepath = checkpoints_dir / filename\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Checkpoint saved: {filepath}\")\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    \"\"\"Load data from checkpoint file\"\"\"\n",
    "    filepath = checkpoints_dir / filename\n",
    "    if filepath.exists():\n",
    "        with open(filepath, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n"
   ],
   "id": "7e0d2cb0a1c1e56f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check if events DataFrame already exists\n",
    "events = load_checkpoint('events.pkl')\n",
    "\n",
    "if events is None:\n",
    "    # Create events DataFrame with timing information\n",
    "    events = pd.DataFrame({\n",
    "        'onset': [0, 60, 120, 180, 240, 300, 360, 420, 480, 540],\n",
    "        'duration': [30] * 10,\n",
    "        'emotion': ['calm', 'afraid', 'delighted', 'depressed',\n",
    "                    'excited', 'delighted', 'depressed', 'calm',\n",
    "                    'excited', 'afraid']\n",
    "    })\n",
    "\n",
    "    # Map emotions to valence categories\n",
    "    valence_mapping = {\n",
    "        'calm': 'neutral',\n",
    "        'afraid': 'negative',\n",
    "        'delighted': 'positive',\n",
    "        'depressed': 'negative',\n",
    "        'excited': 'positive'\n",
    "    }\n",
    "    events['trial_type'] = events['emotion'].map(valence_mapping)\n",
    "\n",
    "    # Save events DataFrame\n",
    "    save_checkpoint(events, 'events.pkl')\n",
    "else:\n",
    "    print(\"Loaded existing events DataFrame\")\n"
   ],
   "id": "f2db7d97ecb1ac21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## Create ROI Masks\n",
    "# Create masks for regions of interest in emotional processing\n"
   ],
   "id": "eef1baa369389199"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check if ROI masks already exist\n",
    "roi_masks = load_checkpoint('roi_masks.pkl')\n",
    "\n",
    "if roi_masks is None:\n",
    "    # Fetch Harvard-Oxford atlases\n",
    "    cortical = fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "    subcortical = fetch_atlas_harvard_oxford('sub-maxprob-thr25-2mm')\n",
    "\n",
    "    # Create ROI masks\n",
    "    roi_masks = {\n",
    "        # Subcortical regions\n",
    "        'amygdala_left': image.math_img('img == 10', img=subcortical.maps),\n",
    "        'amygdala_right': image.math_img('img == 20', img=subcortical.maps),\n",
    "        'hippocampus_left': image.math_img('img == 9', img=subcortical.maps),\n",
    "        'hippocampus_right': image.math_img('img == 19', img=subcortical.maps),\n",
    "\n",
    "        # Cortical regions\n",
    "        'acc': image.math_img('img == 29', img=cortical.maps),\n",
    "        'sup_frontal_gyrus': image.math_img('img == 3', img=cortical.maps),\n",
    "        'mid_frontal_gyrus': image.math_img('img == 4', img=cortical.maps),\n",
    "    }\n",
    "\n",
    "    # Create bilateral masks\n",
    "    bilateral_pairs = [\n",
    "        ('amygdala', 'amygdala_left', 'amygdala_right'),\n",
    "        ('hippocampus', 'hippocampus_left', 'hippocampus_right'),\n",
    "    ]\n",
    "\n",
    "    # Combine bilateral pairs\n",
    "    for name, left, right in bilateral_pairs:\n",
    "        roi_masks[f'{name}_bilateral'] = image.math_img(\n",
    "            \"img1 + img2\",\n",
    "            img1=roi_masks[left],\n",
    "            img2=roi_masks[right]\n",
    "        )\n",
    "\n",
    "    # Save ROI masks\n",
    "    save_checkpoint(roi_masks, 'roi_masks.pkl')\n",
    "else:\n",
    "    print(\"Loaded existing ROI masks\")\n"
   ],
   "id": "b85ae3eeb9117adc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## Process Individual Subjects\n",
    "# Loop through subjects and perform first-level analysis\n"
   ],
   "id": "e7944bc3c4e85dea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check if subject results already exist\n",
    "all_subject_contrasts = load_checkpoint('subject_contrasts.pkl')\n",
    "all_subject_roi_signals = load_checkpoint('subject_roi_signals.pkl')\n",
    "\n",
    "if all_subject_contrasts is None or all_subject_roi_signals is None:\n",
    "    # Initialize lists to store results\n",
    "    n_subjects = 40\n",
    "    all_subject_contrasts = []\n",
    "    all_subject_roi_signals = []\n",
    "\n",
    "    # Process each subject\n",
    "    for subject_id in range(1, n_subjects + 1):\n",
    "        subject_id_str = f'{subject_id:02d}'\n",
    "        print(f'Processing subject {subject_id_str}...')\n",
    "\n",
    "        # Check if individual subject results exist\n",
    "        subject_results = load_checkpoint(f'subject_{subject_id_str}_results.pkl')\n",
    "\n",
    "        if subject_results is None:\n",
    "            # Load subject data\n",
    "            func_file = f'ds005700/sub-{subject_id_str}/func/sub-{subject_id_str}_task-fe_bold.nii.gz'\n",
    "            anat_file = f'ds005700/sub-{subject_id_str}/anat/sub-{subject_id_str}_T1w.nii.gz'\n",
    "\n",
    "            func_img = image.load_img(func_file)\n",
    "            anat_img = image.load_img(anat_file)\n",
    "\n",
    "            # Create and fit first-level model\n",
    "            model = FirstLevelModel(\n",
    "                t_r=2.02697,  # Time repetition from .json file\n",
    "                noise_model='ar1',\n",
    "                standardize=True,\n",
    "                hrf_model='spm',\n",
    "                drift_model='cosine'\n",
    "            )\n",
    "\n",
    "            model.fit(func_img, events)\n",
    "\n",
    "            # Define and compute contrasts\n",
    "            contrasts = {\n",
    "                'positive_vs_neutral': 'positive - neutral',\n",
    "                'negative_vs_neutral': 'negative - neutral',\n",
    "                'positive_vs_negative': 'positive - negative'\n",
    "            }\n",
    "\n",
    "            contrast_maps = {}\n",
    "            for contrast_id, contrast_def in contrasts.items():\n",
    "                contrast_maps[contrast_id] = model.compute_contrast(contrast_def)\n",
    "\n",
    "            # Extract ROI signals\n",
    "            roi_signals = {}\n",
    "            for roi_name, roi_mask in roi_masks.items():\n",
    "                masker = input_data.NiftiMasker(mask_img=roi_mask)\n",
    "                roi_signals[roi_name] = masker.fit_transform(func_img)\n",
    "\n",
    "            # Save individual subject results\n",
    "            subject_results = {'contrasts': contrast_maps, 'roi_signals': roi_signals}\n",
    "            save_checkpoint(subject_results, f'subject_{subject_id_str}_results.pkl')\n",
    "        else:\n",
    "            print(f\"Loaded existing results for subject {subject_id_str}\")\n",
    "            contrast_maps = subject_results['contrasts']\n",
    "            roi_signals = subject_results['roi_signals']\n",
    "\n",
    "        all_subject_contrasts.append(contrast_maps)\n",
    "        all_subject_roi_signals.append(roi_signals)\n",
    "\n",
    "    # Save all subject results\n",
    "    save_checkpoint(all_subject_contrasts, 'subject_contrasts.pkl')\n",
    "    save_checkpoint(all_subject_roi_signals, 'subject_roi_signals.pkl')\n",
    "else:\n",
    "    print(\"Loaded existing subject results\")\n"
   ],
   "id": "81199fc810832d08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## Group Analysis\n",
    "# Perform second-level analysis across all subjects\n"
   ],
   "id": "2249b7654bfdd583"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check if group results already exist\n",
    "group_results = load_checkpoint('group_results.pkl')\n",
    "\n",
    "if group_results is None:\n",
    "    from nilearn.glm import second_level\n",
    "\n",
    "    # Create and fit second-level model\n",
    "    group_model = second_level.SecondLevelModel()\n",
    "\n",
    "    # Compute group results for each contrast\n",
    "    group_results = {}\n",
    "    for contrast_id in all_subject_contrasts[0].keys():\n",
    "        contrast_maps = [subj_contrasts[contrast_id]\n",
    "                         for subj_contrasts in all_subject_contrasts]\n",
    "        group_results[contrast_id] = group_model.fit(contrast_maps).compute_contrast()\n",
    "\n",
    "    # Save group results\n",
    "    save_checkpoint(group_results, 'group_results.pkl')\n",
    "else:\n",
    "    print(\"Loaded existing group results\")\n"
   ],
   "id": "abf0ce8ca3d64f12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ## Plot Results\n",
    "# Generate and save visualization of group-level results\n"
   ],
   "id": "bcc1fdf6de42412"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot and save results for each contrast\n",
    "for contrast_id, stat_map in group_results.items():\n",
    "    output_file = results_dir / f'{contrast_id}_group_map.png'\n",
    "\n",
    "    # Only generate plot if it doesn't exist\n",
    "    if not output_file.exists():\n",
    "        display = plotting.plot_stat_map(\n",
    "            stat_map,\n",
    "            threshold=3.0,\n",
    "            title=contrast_id\n",
    "        )\n",
    "        plt.savefig(output_file)\n",
    "        plt.close()\n",
    "        print(f\"Generated plot: {output_file}\")\n",
    "    else:\n",
    "        print(f\"Plot already exists: {output_file}\")"
   ],
   "id": "70a84ab5e9a26466"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
