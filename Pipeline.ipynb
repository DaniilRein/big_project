{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Initialize the project directory and sub-folders if not done already.\n",
    "\n",
    "Define functions that will be used after each code section to save checkpoints\n",
    "so future runs don't start all over again."
   ],
   "id": "af575672ee8d2682"
  },
  {
   "cell_type": "code",
   "id": "30881c5194c59db5",
   "metadata": {},
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "\"\"\"\n",
    "Create essential directory structure for the project:\n",
    "- fmri_project/: Main project directory\n",
    "- fmri_project/checkpoints/: Store intermediate processing results\n",
    "- fmri_project/results/: Store final analysis outputs\n",
    "\"\"\"\n",
    "project_dir = Path('/Users/daniilsergeyenko/PycharmProjects/fMRI_project')\n",
    "checkpoints_dir = project_dir / 'checkpoints'\n",
    "results_dir = project_dir / 'results'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [checkpoints_dir, results_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(data, filename):\n",
    "    \"\"\"\n",
    "    Save intermediate processing results to avoid unnecessary re-computation.\n",
    "\n",
    "    Args:\n",
    "        data: Any Python object to be saved\n",
    "        filename: Name of the checkpoint file\n",
    "    \"\"\"\n",
    "    filepath = checkpoints_dir / filename\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Checkpoint saved: {filepath}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    \"\"\"\n",
    "    Load previously saved processing results.\n",
    "\n",
    "    Args:\n",
    "        filename: Name of the checkpoint file\n",
    "    Returns:\n",
    "        Loaded data if checkpoint exists, None otherwise\n",
    "    \"\"\"\n",
    "    filepath = checkpoints_dir / filename\n",
    "    if filepath.exists():\n",
    "        with open(filepath, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create the events data frame based on the original data parameters and my unique research question.",
   "id": "736aaa6087c2981e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check if events DataFrame already exists\n",
    "events = load_checkpoint('events.pkl')\n",
    "\n",
    "if events is None:\n",
    "    # The original data is sequenced as follows:\n",
    "    # 'emotion': ['calm', 'afraid', 'delighted', 'depressed',\n",
    "    #             'excited', 'delighted', 'depressed', 'calm',\n",
    "    #             'excited', 'afraid']\n",
    "    # Mapping instead to the groups I selected; neutral, positive, negative.\n",
    "    # Then removing the redundant emotion column to avoid runtime UserWarning.\n",
    "\n",
    "    events = pd.DataFrame({\n",
    "        'onset': [0, 60, 120, 180, 240, 300, 360, 420, 480, 540],\n",
    "        'duration': [30] * 10,\n",
    "        'trial_type': ['neutral', 'negative', 'positive', 'negative',\n",
    "                      'positive', 'positive', 'negative', 'neutral',\n",
    "                      'positive', 'negative']\n",
    "    })\n",
    "\n",
    "    # Save events DataFrame\n",
    "    save_checkpoint(events, 'events.pkl')\n",
    "else:\n",
    "    print(\"Loaded existing events DataFrame\")"
   ],
   "id": "ccde05bd00c3f203",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the single-subject analysis for each participant and save the results.",
   "id": "b74bb8fd43be654a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nilearn import image\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "\n",
    "# Check if subject results already exist\n",
    "all_subject_contrasts = load_checkpoint('subject_contrasts.pkl')\n",
    "\n",
    "if all_subject_contrasts is None:\n",
    "    # I use open data of 40 participants.\n",
    "    n_subjects = 40\n",
    "    # Initialize lists to store results.\n",
    "    all_subject_contrasts = []\n",
    "\n",
    "    # Process each subject\n",
    "    for subject_id in range(1, n_subjects + 1):\n",
    "        subject_id_str = f'{subject_id:02d}'\n",
    "        print(f'Processing subject {subject_id_str}...')\n",
    "\n",
    "        # Check if individual subject results exist\n",
    "        subject_results = load_checkpoint(f'subject_{subject_id_str}_results.pkl')\n",
    "\n",
    "        if subject_results is None:\n",
    "            # Load the subject's functional data\n",
    "            func_file = f'ds005700/sub-{subject_id_str}/func/sub-{subject_id_str}_task-fe_bold.nii.gz'\n",
    "            func_img = image.load_img(func_file)\n",
    "\n",
    "            # Create and fit first-level model. With explicit parameters.\n",
    "            model = FirstLevelModel(\n",
    "                t_r=2.02697,  # Time(sec) it takes to scan one full head volume. written in each .json file.\n",
    "                noise_model='ar1',\n",
    "                standardize=True,\n",
    "                hrf_model='spm',\n",
    "                drift_model='cosine'\n",
    "            )\n",
    "            model.fit(func_img, events)\n",
    "\n",
    "            # Define and compute contrasts\n",
    "            contrasts = {\n",
    "                'positive_vs_neutral': 'positive - neutral',\n",
    "                'negative_vs_neutral': 'negative - neutral',\n",
    "                'positive_vs_negative': 'positive - negative'\n",
    "            }\n",
    "            contrast_maps = {}\n",
    "            for contrast_id, contrast_def in contrasts.items():\n",
    "                contrast_maps[contrast_id] = model.compute_contrast(contrast_def)\n",
    "\n",
    "            # Save individual subject results\n",
    "            subject_results = {'contrasts': contrast_maps}\n",
    "            save_checkpoint(subject_results, f'subject_{subject_id_str}_results.pkl')\n",
    "        else:\n",
    "            # in case the specific subject's results already exist fetch them instead.\n",
    "            print(f\"Loaded existing results for subject {subject_id_str}\")\n",
    "            contrast_maps = subject_results['contrasts']\n",
    "\n",
    "        all_subject_contrasts.append(contrast_maps)\n",
    "\n",
    "    # Save all subject results\n",
    "    save_checkpoint(all_subject_contrasts, 'subject_contrasts.pkl')\n",
    "else:\n",
    "    print(\"Loaded existing subject results\")"
   ],
   "id": "84bf839bb8ff0bcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run the group analysis, averaging all the participants and saving the results.",
   "id": "22c719e54b5cfde6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn import plotting, image\n",
    "\n",
    "# Check if group results already exist\n",
    "group_results = load_checkpoint('group_results.pkl')\n",
    "\n",
    "if group_results is None:\n",
    "    # Create design matrix for second-level analysis\n",
    "    n_subjects = 40\n",
    "    design_matrix = pd.DataFrame({\n",
    "        'intercept': np.ones(n_subjects),\n",
    "    })\n",
    "\n",
    "    # Initialize second-level model with explicit parameters\n",
    "    second_level_model = SecondLevelModel(\n",
    "        smoothing_fwhm=8.0,  # Add explicit smoothing\n",
    "        n_jobs=1,  # Ensure single job for stability\n",
    "        memory=None,  # Don't cache computations\n",
    "        verbose=0  # Reduce verbosity\n",
    "    )\n",
    "\n",
    "    # Define contrasts to analyze - same as within each subject.\n",
    "    contrasts_to_analyze = [\n",
    "        'positive_vs_neutral',\n",
    "        'negative_vs_neutral',\n",
    "        'positive_vs_negative'\n",
    "    ]\n",
    "\n",
    "    # Initialize dictionary to store results for each of the three contrasts.\n",
    "    group_results = {}\n",
    "\n",
    "    # Process each contrast\n",
    "    for contrast_name in contrasts_to_analyze:\n",
    "        print(f\"Running group analysis for {contrast_name}...\")\n",
    "\n",
    "        # Extract and preprocess contrast maps for all subjects\n",
    "        contrast_maps_list = []\n",
    "\n",
    "        # Get the first map to use as reference\n",
    "        first_map = all_subject_contrasts[0][contrast_name]\n",
    "        reference_affine = first_map.affine\n",
    "        reference_shape = first_map.shape[:3]  # Exclude time dimension if present\n",
    "\n",
    "        # Process each subject's contrast map\n",
    "        for subject_contrasts in all_subject_contrasts:\n",
    "            contrast_img = subject_contrasts[contrast_name]\n",
    "\n",
    "            # Ensure 3D\n",
    "            if len(contrast_img.shape) == 4:\n",
    "                contrast_img = image.index_img(contrast_img, 0)\n",
    "\n",
    "            # Resample to match reference with explicit parameters\n",
    "            resampled_img = image.resample_img(\n",
    "                contrast_img,\n",
    "                target_affine=reference_affine,\n",
    "                target_shape=reference_shape,\n",
    "                interpolation='continuous',\n",
    "                force_resample=True,\n",
    "                copy_header=True,\n",
    "                clip=True\n",
    "            )\n",
    "\n",
    "            contrast_maps_list.append(resampled_img)\n",
    "\n",
    "        # Attempt to run the second_level_model, utilizing try;except in case the previous processes weren't saved correctly.\n",
    "        try:\n",
    "            # Fit the second-level model\n",
    "            second_level_model.fit(contrast_maps_list, design_matrix=design_matrix)\n",
    "\n",
    "            # Compute the group-level contrast\n",
    "            z_map = second_level_model.compute_contrast(\n",
    "                output_type='z_score',\n",
    "                second_level_stat_type='t'  # Explicitly specify t-test\n",
    "            )\n",
    "            p_map = second_level_model.compute_contrast(\n",
    "                output_type='p_value',\n",
    "                second_level_stat_type='t'\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            group_results[contrast_name] = (z_map, p_map)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing contrast {contrast_name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Save group results\n",
    "    save_checkpoint(group_results, 'group_results.pkl')\n",
    "else:\n",
    "    print(\"Loaded existing group results\")"
   ],
   "id": "15e2107e117009ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plotting the results of all contrast comparisons.",
   "id": "1d6aeef637234b3e"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Plotting parameters\n",
    "z_threshold = 3.1  # Corresponds to p < 0.001\n",
    "p_val = norm.sf(z_threshold)  # Convert Z-score to p-value\n",
    "print(f\"Z-score threshold {z_threshold} corresponds to p-value < {p_val:.3e}\")\n",
    "\n",
    "# Plot results for each contrast\n",
    "for contrast_name, (z_map, p_map) in group_results.items():\n",
    "    print(f\"Plotting results for {contrast_name}...\")\n",
    "\n",
    "    # Create figure with multiple views\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot sagittal, coronal, and axial views with statistical threshold\n",
    "    display = plotting.plot_stat_map(\n",
    "        z_map,\n",
    "        threshold=z_threshold,  # Apply threshold only during visualization\n",
    "        display_mode='ortho',\n",
    "        title=f'Group-level {contrast_name}\\n'\n",
    "              f'(threshold: z>{z_threshold}, p<{p_val:.3e})',\n",
    "        figure=fig\n",
    "    )\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f'results/group_analysis_{contrast_name}(Z scores).png',\n",
    "                dpi=300,\n",
    "                bbox_inches='tight')\n",
    "    plt.close()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
